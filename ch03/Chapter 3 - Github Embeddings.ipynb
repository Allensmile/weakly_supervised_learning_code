{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Github Embeddings\n",
    "\n",
    "In this notebook we're going to go beyond using pre-trained embeddings and models we download from the internet and start to create our own secondary models that can improve the primary model through transfer learning. We're going to train text and code embeddings based on Github's [CodeSearchNet](https://github.com/rjurney/CodeSearchNet) datasets. They include both doc strings and code for 2 million posts and while they use the data to map from text search queries to code, we'll be using it to create separate [BERT](https://arxiv.org/abs/1810.04805) embeddings to drive our Stack Overflow tagger.\n",
    "\n",
    "The paper for CodeSearchNet is on arXiv at [CodeSearchNet Challenge: Evaluating the State of Semantic Code Search](https://arxiv.org/abs/1909.09436)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(1337)\n",
    "\n",
    "# Add parent directory to path\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from lib.utils import extract_text_plain\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CodeSearchNet Data\n",
    "\n",
    "We load the entire CodeSearchNet dataset for Go, Java, PHP, Python and Ruby. While the code doesn't cover all languages I'm hoping they are diverse enough to handle other languages and so will still help performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OptionError",
     "evalue": "\"No such keys(s): 'display.html.border'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOptionError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/weak/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weak/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"<pre>\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"</pre>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.notebook_repr_html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m             \u001b[0mmax_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.max_rows\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mmin_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.min_rows\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weak/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_html\u001b[0;34m(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, max_rows, max_cols, show_dimensions, decimal, bold_rows, classes, escape, notebook, border, table_id, render_links)\u001b[0m\n\u001b[1;32m   2263\u001b[0m             \u001b[0mMake\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrow\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0mbold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2264\u001b[0m         \u001b[0mclasses\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlist\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2265\u001b[0;31m             \u001b[0mCSS\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mto\u001b[0m \u001b[0mapply\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresulting\u001b[0m \u001b[0mhtml\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2266\u001b[0m         \u001b[0mescape\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2267\u001b[0m             \u001b[0mConvert\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcharacters\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mto\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msafe\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weak/lib/python3.7/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_html\u001b[0;34m(self, classes, notebook, border)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_show_dimensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             self.buf.write(\n\u001b[0m\u001b[1;32m    730\u001b[0m                 \"\\n\\n[{nrows} rows x {ncols} columns]\".format(\n\u001b[1;32m    731\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weak/lib/python3.7/site-packages/pandas/io/formats/html.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, formatter, classes, border)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_dimensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mborder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mborder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.html.border\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mborder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mborder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weak/lib/python3.7/site-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__func__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weak/lib/python3.7/site-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m_get_option\u001b[0;34m(pat, silent)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_single_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# walk the nested dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weak/lib/python3.7/site-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m_get_single_key\u001b[0;34m(pat, silent)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0m_warn_if_deprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOptionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No such keys(s): {pat!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mOptionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pattern matched multiple keys\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOptionError\u001b[0m: \"No such keys(s): 'display.html.border'\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                code  \\\n",
       "0  protected final void fastPathOrderedEmit(U val...   \n",
       "1  @CheckReturnValue\\n    @NonNull\\n    @Schedule...   \n",
       "2  @SuppressWarnings(\"unchecked\")\\n    @CheckRetu...   \n",
       "3  @SuppressWarnings({ \"unchecked\", \"rawtypes\" })...   \n",
       "4  @SuppressWarnings({ \"unchecked\", \"rawtypes\" })...   \n",
       "\n",
       "                                         code_tokens  \\\n",
       "0  [protected, final, void, fastPathOrderedEmit, ...   \n",
       "1  [@, CheckReturnValue, @, NonNull, @, Scheduler...   \n",
       "2  [@, SuppressWarnings, (, \"unchecked\", ), @, Ch...   \n",
       "3  [@, SuppressWarnings, (, {, \"unchecked\", ,, \"r...   \n",
       "4  [@, SuppressWarnings, (, {, \"unchecked\", ,, \"r...   \n",
       "\n",
       "                                           docstring  \\\n",
       "0  Makes sure the fast-path emits in order.\\n@par...   \n",
       "1  Mirrors the one ObservableSource in an Iterabl...   \n",
       "2  Mirrors the one ObservableSource in an array o...   \n",
       "3  Concatenates elements of each ObservableSource...   \n",
       "4  Returns an Observable that emits the items emi...   \n",
       "\n",
       "                                    docstring_tokens  \\\n",
       "0  [Makes, sure, the, fast, -, path, emits, in, o...   \n",
       "1  [Mirrors, the, one, ObservableSource, in, an, ...   \n",
       "2  [Mirrors, the, one, ObservableSource, in, an, ...   \n",
       "3  [Concatenates, elements, of, each, ObservableS...   \n",
       "4  [Returns, an, Observable, that, emits, the, it...   \n",
       "\n",
       "                                func_name language  \\\n",
       "0  QueueDrainObserver.fastPathOrderedEmit     java   \n",
       "1                          Observable.amb     java   \n",
       "2                     Observable.ambArray     java   \n",
       "3                       Observable.concat     java   \n",
       "4                       Observable.concat     java   \n",
       "\n",
       "                                     original_string partition  \\\n",
       "0  protected final void fastPathOrderedEmit(U val...      test   \n",
       "1  @CheckReturnValue\\n    @NonNull\\n    @Schedule...      test   \n",
       "2  @SuppressWarnings(\"unchecked\")\\n    @CheckRetu...      test   \n",
       "3  @SuppressWarnings({ \"unchecked\", \"rawtypes\" })...      test   \n",
       "4  @SuppressWarnings({ \"unchecked\", \"rawtypes\" })...      test   \n",
       "\n",
       "                                                path              repo  \\\n",
       "0  src/main/java/io/reactivex/internal/observers/...  ReactiveX/RxJava   \n",
       "1         src/main/java/io/reactivex/Observable.java  ReactiveX/RxJava   \n",
       "2         src/main/java/io/reactivex/Observable.java  ReactiveX/RxJava   \n",
       "3         src/main/java/io/reactivex/Observable.java  ReactiveX/RxJava   \n",
       "4         src/main/java/io/reactivex/Observable.java  ReactiveX/RxJava   \n",
       "\n",
       "                                        sha  \\\n",
       "0  ac84182aa2bd866b53e01c8e3fe99683b882c60e   \n",
       "1  ac84182aa2bd866b53e01c8e3fe99683b882c60e   \n",
       "2  ac84182aa2bd866b53e01c8e3fe99683b882c60e   \n",
       "3  ac84182aa2bd866b53e01c8e3fe99683b882c60e   \n",
       "4  ac84182aa2bd866b53e01c8e3fe99683b882c60e   \n",
       "\n",
       "                                                 url  \n",
       "0  https://github.com/ReactiveX/RxJava/blob/ac841...  \n",
       "1  https://github.com/ReactiveX/RxJava/blob/ac841...  \n",
       "2  https://github.com/ReactiveX/RxJava/blob/ac841...  \n",
       "3  https://github.com/ReactiveX/RxJava/blob/ac841...  \n",
       "4  https://github.com/ReactiveX/RxJava/blob/ac841...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "# Load all Gzipped JSON Lines files in the data directory\n",
    "for filename in Path('../data/CodeSearchNet').glob('**/*.jsonl.gz'):\n",
    "    new_df = pd.read_json(filename, lines=True)\n",
    "    df = pd.concat([df, new_df])\n",
    "    \n",
    "    # Carefully manage memory\n",
    "    del new_df\n",
    "    gc.collect()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2,070,536 functions\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'There are {len(df[\"docstring\"].index):,} functions'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text from Docstrings\n",
    "\n",
    "Docstings can contain HTML, so we parse them and extract text using `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = df['code']\n",
    "docs = df.docstring.apply(lambda x: extract_text_plain(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the result of the code removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 500)\n",
    "doc_df = pd.DataFrame({'docs': docs, 'docstring': df['docstring']})\n",
    "\n",
    "doc_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate CSV for BERT\n",
    "\n",
    "The [Google BERT Github project](https://github.com/google-research/bert) is a submodule to this project, which you can checkout from within this [cloned project](https://github.com/rjurney/weakly_supervised_learning_code) with:\n",
    "\n",
    "```bash\n",
    "git submodule init\n",
    "git submodule update\n",
    "```\n",
    "\n",
    "We need to generate CSV in the format that BERT expects, which is:\n",
    "\n",
    "> Here's how to run the data generation. The input is a plain text file, with one sentence per line. (It is important that these be actual sentences for the \"next sentence prediction\" task). Documents are delimited by empty lines. The output is a set of tf.train.Examples serialized into TFRecord file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenizer = PunktSentenceTokenizer()\n",
    "sentences = docs.apply(sentence_tokenizer.tokenize)\n",
    "\n",
    "sentences.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/sentences.csv', 'w') as f:\n",
    "    \n",
    "    current_idx = 0\n",
    "    for idx, doc in sentences.items():\n",
    "        # Insert a newline to separate documents\n",
    "        if idx != current_idx:\n",
    "            f.write('\\n')\n",
    "        # Write each sentence exactly as it appared to one line each\n",
    "        for sentence in doc:\n",
    "            f.write(sentence.encode('unicode-escape').decode().replace('\\\\\\\\', '\\\\') + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `sentencepiece` to Extract a WordPiece Vocabulary\n",
    "\n",
    "BERT needs a WordPiece vocabulary file to run, so we need to decide on a number of tokens and then run `sentencepiece` to extract a list of valid tokens.\n",
    "\n",
    "The `sentencepiece` Pypi library isn't sufficient for our needs, we need to clone the Github repo, build and install the software to create our vocabulary.\n",
    "\n",
    "Make sure you're in the root directory of this project and run:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/google/sentencepiece\n",
    "cd sentencepiece\n",
    "\n",
    "mkdir build\n",
    "cd build\n",
    "cmake ..\n",
    "make -j $(nproc)\n",
    "sudo make install\n",
    "sudo ldconfig -v\n",
    "```\n",
    "\n",
    "Now we can use `sp_train` to create a vocabulary of our 4.7 million sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(49) LOG(INFO) Starts training with : \n",
      "TrainerSpec {\n",
      "  input: ../data/sentences.csv\n",
      "  input_format: \n",
      "  model_prefix: wsl\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "NormalizerSpec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "\n",
      "trainer_interface.cc(267) LOG(INFO) Loading corpus: ../data/sentences.csv\n",
      "trainer_interface.cc(287) LOG(WARNING) Found too long line (10137 > 4192).\n",
      "trainer_interface.cc(289) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(290) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(139) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(139) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(139) LOG(INFO) Loaded 3000000 lines\n",
      "trainer_interface.cc(139) LOG(INFO) Loaded 4000000 lines\n",
      "trainer_interface.cc(114) LOG(WARNING) Too many sentences are loaded! (4672311), which may slow down training.\n",
      "trainer_interface.cc(116) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(119) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(315) LOG(INFO) Loaded all 4672311 sentences\n",
      "trainer_interface.cc(321) LOG(INFO) Skipped 138 too long sentences.\n",
      "trainer_interface.cc(330) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(330) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(330) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(335) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(384) LOG(INFO) all chars count=415059453\n",
      "trainer_interface.cc(392) LOG(INFO) Done: 99.9539% characters are covered.\n",
      "trainer_interface.cc(402) LOG(INFO) Alphabet size=87\n",
      "trainer_interface.cc(403) LOG(INFO) Final character coverage=0.999539\n",
      "trainer_interface.cc(435) LOG(INFO) Done! preprocessed 4672311 sentences.\n",
      "tcmalloc: large alloc 1073741824 bytes == 0x5599302e4000 @ \n",
      "tcmalloc: large alloc 2147483648 bytes == 0x5599702e4000 @ \n",
      "tcmalloc: large alloc 1678934016 bytes == 0x5598f0324000 @ \n",
      "tcmalloc: large alloc 1678934016 bytes == 0x5599f0b3e000 @ \n",
      "unigram_model_trainer.cc(129) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(133) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(184) LOG(INFO) Initialized 1000000 seed sentencepieces\n",
      "trainer_interface.cc(441) LOG(INFO) Tokenizing input sentences with whitespace: 4672311\n",
      "trainer_interface.cc(451) LOG(INFO) Done! 2913459\n",
      "unigram_model_trainer.cc(470) LOG(INFO) Using 2913459 sentences for EM training\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=402191 obj=15.8716 num_tokens=17565242 num_tokens/piece=43.6739\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=290028 obj=12.3459 num_tokens=17761916 num_tokens/piece=61.2421\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=217379 obj=12.3109 num_tokens=17939149 num_tokens/piece=82.5248\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=216928 obj=12.3005 num_tokens=17963884 num_tokens/piece=82.8104\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=162642 obj=12.3125 num_tokens=18127355 num_tokens/piece=111.456\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=162593 obj=12.3097 num_tokens=18134378 num_tokens/piece=111.532\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=121903 obj=12.3328 num_tokens=18354369 num_tokens/piece=150.565\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=121873 obj=12.3308 num_tokens=18355456 num_tokens/piece=150.611\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=91388 obj=12.3646 num_tokens=18588438 num_tokens/piece=203.401\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=91369 obj=12.3622 num_tokens=18589120 num_tokens/piece=203.451\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=68516 obj=12.4059 num_tokens=18839927 num_tokens/piece=274.971\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=68507 obj=12.4042 num_tokens=18848121 num_tokens/piece=275.127\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=51371 obj=12.4584 num_tokens=19122421 num_tokens/piece=372.242\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=51365 obj=12.4555 num_tokens=19122671 num_tokens/piece=372.29\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=38519 obj=12.5226 num_tokens=19429140 num_tokens/piece=504.404\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=38516 obj=12.5172 num_tokens=19428420 num_tokens/piece=504.425\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=28886 obj=12.604 num_tokens=19792690 num_tokens/piece=685.2\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=28885 obj=12.5884 num_tokens=19792714 num_tokens/piece=685.225\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=21999 obj=12.6968 num_tokens=20199805 num_tokens/piece=918.215\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=21998 obj=12.6785 num_tokens=20199128 num_tokens/piece=918.226\n",
      "trainer_interface.cc(507) LOG(INFO) Saving model: wsl.model\n",
      "trainer_interface.cc(531) LOG(INFO) Saving vocabs: wsl.vocab\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd ../models\n",
    "spm_train --input=\"../data/sentences.csv\" --model_prefix=wsl --vocab_size=20000\n",
    "\n",
    "# Add the [CLS], [SEP], [UNK] and [MASK] tags, or pre-training will error out\n",
    "echo -e \"[CLS]\\t0\\n[SEP]\\t0\\n[UNK]\\t0\\n[MASK]\\t0\\n$(cat wsl.vocab)\" > wsl.vocab\n",
    "\n",
    "# Remove the numbers, just retain the tag vocabulary\n",
    "cat wsl.vocab | cut -d$'\\t' -f1 > wsl.stripped.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT to Pretrain a Language Model\n",
    "\n",
    "Next we use the WordPiece vocabulary to pre-train a BERT model that we will then use, as a tranfer learning strategy, to encode the text of Stack Overflow questions.\n",
    "\n",
    "### Creating a BERT conda environment\n",
    "\n",
    "It is not possible to create a new conda environment from which to install `tensorflow==1.14.0`, which BERT needs, so you will need to run this code outside of this notebook, from the root directory of this project.\n",
    "\n",
    "\n",
    "```bash\n",
    "conda create -y -n bert python=3.7.4\n",
    "conda init bash\n",
    "```\n",
    "\n",
    "Now in a new shell, change directory to the root of project:\n",
    "\n",
    "```bash\n",
    "cd /path/to/weakly_supervised_learning_code\n",
    "```\n",
    "\n",
    "Now run:\n",
    "\n",
    "```bash\n",
    "conda activate bert\n",
    "pip install tensorflow-gpu==1.14.0\n",
    "```\n",
    "\n",
    "### Creating BERT Pre-Training Data\n",
    "\n",
    "Before we can train a BERT model or extract static embedding values we need to create the pre-training data the model uses to train. The output file will be 20GB, so make sure you have the space available!\n",
    "\n",
    "From the [BERT README](https://github.com/google-research/bert/blob/master/README.md):\n",
    "\n",
    "> Here's how to run the data generation. The input is a plain text file, with one sentence per line. (It is important that these be actual sentences for the \"next sentence prediction\" task). Documents are delimited by empty lines. The output is a set of tf.train.Examples serialized into TFRecord file format.\n",
    "\n",
    "We need to configure BERT to use our vocabulary size, so we create a `bert_config.json` file in the `bert/` directory.\n",
    "\n",
    "```bash\n",
    "# Tell BERT how many tokens to use\n",
    "echo '{ \"vocab_size\": 20004 }' > bert/bert_config.json \n",
    "```\n",
    "\n",
    "Then we execute the `create_pretraining_data.py` command to pre-train the network.\n",
    "\n",
    "```bash\n",
    "python bert/create_pretraining_data.py \\\n",
    "   --input_file=data/sentences.csv \\\n",
    "   --output_file=data/tf_examples.tfrecord \\\n",
    "   --vocab_file=models/wsl.stripped.vocab \\\n",
    "   --bert_config_file=bert/bert_config.json \\\n",
    "   --do_lower_case=False \\\n",
    "   --max_seq_length=128 \\\n",
    "   --max_predictions_per_seq=20 \\\n",
    "   --num_train_steps=20 \\\n",
    "   --num_warmup_steps=10 \\\n",
    "   --random_seed=1337 \\\n",
    "   --learning_rate=2e-5\n",
    "```\n",
    "\n",
    "Now we can run pretraining. If your GPU is only 8GB of RAM, reduce the training batch size to 16 or 24.\n",
    "\n",
    "```bash\n",
    "python bert/run_pretraining.py \\\n",
    "  --input_file=data/tf_examples.tfrecord \\\n",
    "  --output_dir=models/pretraining_output \\\n",
    "  --do_train=True \\\n",
    "  --do_eval=True \\\n",
    "  --bert_config_file=bert/bert_config.json \\\n",
    "  --train_batch_size=32 \\\n",
    "  --max_seq_length=128 \\\n",
    "  --max_predictions_per_seq=20 \\\n",
    "  --num_train_steps=10000 \\\n",
    "  --num_warmup_steps=10 \\\n",
    "  --learning_rate=2e-5\n",
    "```\n",
    "\n",
    "Finally, deactivate the conda environment:\n",
    "\n",
    "```bash\n",
    "conda deactivate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
