{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Github Embeddings\n",
    "\n",
    "In this notebook we're going to go beyond using pre-trained embeddings and models we download from the internet and start to create our own secondary models that can improve the primary model through transfer learning. We're going to train text and code embeddings based on Github's [CodeSearchNet](https://github.com/rjurney/CodeSearchNet) datasets. They include both doc strings and code for 2 million posts and while they use the data to map from text search queries to code, we'll be using it to create separate [BERT](https://arxiv.org/abs/1810.04805) embeddings to drive our Stack Overflow tagger.\n",
    "\n",
    "The paper for CodeSearchNet is on arXiv at [CodeSearchNet Challenge: Evaluating the State of Semantic Code Search](https://arxiv.org/abs/1909.09436)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(1337)\n",
    "\n",
    "# Add parent directory to path\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from lib.utils import extract_text_plain\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CodeSearchNet Data\n",
    "\n",
    "We load the entire CodeSearchNet dataset for Go, Java, PHP, Python and Ruby. While the code doesn't cover all languages I'm hoping they are diverse enough to handle other languages and so will still help performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "# Load all Gzipped JSON Lines files in the data directory\n",
    "for filename in Path('../data/CodeSearchNet').glob('**/*.jsonl.gz'):\n",
    "    new_df = pd.read_json(filename, lines=True)\n",
    "    df = pd.concat([df, new_df])\n",
    "    \n",
    "    # Carefully manage memory\n",
    "    del new_df\n",
    "    gc.collect()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'There are {len(df[\"docstring\"].index):,} functions'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text from Docstrings\n",
    "\n",
    "Docstings can contain HTML, so we parse them and extract text using `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = df['code']\n",
    "docs = df.docstring.apply(lambda x: extract_text_plain(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the result of the code removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 500)\n",
    "doc_df = pd.DataFrame({'docs': docs, 'docstring': df['docstring']})\n",
    "\n",
    "doc_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About BERT\n",
    "\n",
    "Google BERT is described in the [BERT README](https://github.com/google-research/bert/blob/master/README.md):\n",
    "\n",
    "> BERT is a method of pre-training language representations, meaning that we train a general-purpose \"language understanding\" model on a large text corpus (like Wikipedia), and then use that model for downstream NLP tasks that we care about (like question answering). BERT outperforms previous methods because it is the first unsupervised, deeply bidirectional system for pre-training NLP.\n",
    "\n",
    "> Unsupervised means that BERT was trained using only a plain text corpus, which is important because an enormous amount of plain text data is publicly available on the web in many languages.\n",
    "\n",
    "> Pre-trained representations can also either be context-free or contextual, and contextual representations can further be unidirectional or bidirectional. Context-free models such as word2vec or GloVe generate a single \"word embedding\" representation for each word in the vocabulary, so bank would have the same representation in bank deposit and river bank. Contextual models instead generate a representation of each word that is based on the other words in the sentence.\n",
    "\n",
    "> BERT was built upon recent work in pre-training contextual representations — including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, and ULMFit — but crucially these models are all unidirectional or shallowly bidirectional. This means that each word is only contextualized using the words to its left (or right). For example, in the sentence I made a bank deposit the unidirectional representation of bank is only based on I made a but not deposit. Some previous work does combine the representations from separate left-context and right-context models, but only in a \"shallow\" manner. BERT represents \"bank\" using both its left and right context — I made a ... deposit — starting from the very bottom of a deep neural network, so it is deeply bidirectional.\n",
    "\n",
    "## Generate CSV for BERT\n",
    "\n",
    "The [Google BERT Github project](https://github.com/google-research/bert) is a submodule to this project, which you can checkout from within this [cloned project](https://github.com/rjurney/weakly_supervised_learning_code) with:\n",
    "\n",
    "```bash\n",
    "git submodule init\n",
    "git submodule update\n",
    "```\n",
    "\n",
    "We need to generate CSV in the format that BERT expects, which is:\n",
    "\n",
    "> Here's how to run the data generation. The input is a plain text file, with one sentence per line. (It is important that these be actual sentences for the \"next sentence prediction\" task). Documents are delimited by empty lines. The output is a set of tf.train.Examples serialized into TFRecord file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenizer = PunktSentenceTokenizer()\n",
    "sentences = docs.apply(sentence_tokenizer.tokenize)\n",
    "\n",
    "sentences.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/sentences.csv', 'w') as f:\n",
    "    \n",
    "    current_idx = 0\n",
    "    for idx, doc in sentences.items():\n",
    "        # Insert a newline to separate documents\n",
    "        if idx != current_idx:\n",
    "            f.write('\\n')\n",
    "        # Write each sentence exactly as it appared to one line each\n",
    "        for sentence in doc:\n",
    "            f.write(sentence.encode('unicode-escape').decode().replace('\\\\\\\\', '\\\\') + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `sentencepiece` to Extract a WordPiece Vocabulary\n",
    "\n",
    "BERT needs a WordPiece vocabulary file to run, so we need to decide on a number of tokens and then run `sentencepiece` to extract a list of valid tokens.\n",
    "\n",
    "The `sentencepiece` Pypi library isn't sufficient for our needs, we need to clone the Github repo, build and install the software to create our vocabulary.\n",
    "\n",
    "Make sure you're in the root directory of this project and run:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/google/sentencepiece\n",
    "cd sentencepiece\n",
    "\n",
    "mkdir build\n",
    "cd build\n",
    "cmake ..\n",
    "make -j $(nproc)\n",
    "sudo make install\n",
    "sudo ldconfig -v\n",
    "```\n",
    "\n",
    "Now we can use `sp_train` to create a vocabulary of our 4.7 million sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd ../models\n",
    "spm_train --input=\"../data/sentences.csv\" --model_prefix=wsl --vocab_size=20000\n",
    "\n",
    "# Add the [CLS], [SEP], [UNK] and [MASK] tags, or pre-training will error out\n",
    "echo -e \"[CLS]\\t0\\n[SEP]\\t0\\n[UNK]\\t0\\n[MASK]\\t0\\n$(cat wsl.vocab)\" > wsl.vocab\n",
    "\n",
    "# Remove the numbers, just retain the tag vocabulary\n",
    "cat wsl.vocab | cut -d$'\\t' -f1 > wsl.stripped.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT to Pretrain a Language Model\n",
    "\n",
    "Next we use the WordPiece vocabulary to pre-train a BERT model that we will then use, as a tranfer learning strategy, to encode the text of Stack Overflow questions.\n",
    "\n",
    "### Creating a BERT conda environment\n",
    "\n",
    "It is not possible to create a new conda environment from which to install `tensorflow==1.14.0`, which BERT needs, so you will need to run this code outside of this notebook, from the root directory of this project.\n",
    "\n",
    "\n",
    "```bash\n",
    "conda create -y -n bert python=3.7.4\n",
    "conda init bash\n",
    "```\n",
    "\n",
    "Now in a new shell, change directory to the root of project:\n",
    "\n",
    "```bash\n",
    "cd /path/to/weakly_supervised_learning_code\n",
    "```\n",
    "\n",
    "Now run:\n",
    "\n",
    "```bash\n",
    "conda activate bert\n",
    "pip install tensorflow-gpu==1.14.0\n",
    "```\n",
    "\n",
    "### Creating BERT Pre-Training Data\n",
    "\n",
    "Before we can train a BERT model or extract static embedding values we need to create the pre-training data the model uses to train. The output file will be 20GB, so make sure you have the space available!\n",
    "\n",
    "From the [BERT README](https://github.com/google-research/bert/blob/master/README.md):\n",
    "\n",
    "> Here's how to run the data generation. The input is a plain text file, with one sentence per line. (It is important that these be actual sentences for the \"next sentence prediction\" task). Documents are delimited by empty lines. The output is a set of tf.train.Examples serialized into TFRecord file format.\n",
    "\n",
    "We need to configure BERT to use our vocabulary size, so we create a `bert_config.json` file in the `bert/` directory.\n",
    "\n",
    "```bash\n",
    "# Tell BERT how many tokens to use\n",
    "echo '{ \"vocab_size\": 20004 }' > bert/bert_config.json \n",
    "```\n",
    "\n",
    "Then we execute the `create_pretraining_data.py` command to pre-train the network.\n",
    "\n",
    "```bash\n",
    "python bert/create_pretraining_data.py \\\n",
    "   --input_file=data/sentences.csv \\\n",
    "   --output_file=data/tf_examples.tfrecord \\\n",
    "   --vocab_file=models/wsl.stripped.vocab \\\n",
    "   --bert_config_file=bert/bert_config.json \\\n",
    "   --do_lower_case=False \\\n",
    "   --max_seq_length=128 \\\n",
    "   --max_predictions_per_seq=20 \\\n",
    "   --num_train_steps=20 \\\n",
    "   --num_warmup_steps=10 \\\n",
    "   --random_seed=1337 \\\n",
    "   --learning_rate=2e-5\n",
    "```\n",
    "\n",
    "Now we can run pretraining. If your GPU is only 8GB of RAM, reduce the training batch size to 16 or 24.\n",
    "\n",
    "```bash\n",
    "python bert/run_pretraining.py \\\n",
    "  --input_file=data/tf_examples.tfrecord \\\n",
    "  --output_dir=models/bert_pretraining_output \\\n",
    "  --do_train=True \\\n",
    "  --do_eval=True \\\n",
    "  --bert_config_file=bert/bert_config.json \\\n",
    "  --train_batch_size=32 \\\n",
    "  --max_seq_length=128 \\\n",
    "  --max_predictions_per_seq=20 \\\n",
    "  --num_train_steps=10000 \\\n",
    "  --num_warmup_steps=10 \\\n",
    "  --learning_rate=2e-5\n",
    "```\n",
    "\n",
    "Finally, deactivate the conda environment:\n",
    "\n",
    "```bash\n",
    "conda deactivate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
