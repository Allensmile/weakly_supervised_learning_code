{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Snorkel\n",
    "\n",
    "In this notebook we will use Snorkel to enrich our data such that tags with between 500-2,000 examples will be labeled using weak supervision to produce labels for enough examples to allow us to train an accurate full model that includes these new labels.\n",
    "\n",
    "More information about Snorkel can be found at [Snorkel.org](https://www.snorkel.org/) :) For a basic introduction to Snorkel, see the [Spam Tutorial](http://syndrome:8888/notebooks/snorkel-tutorials/spam/01_spam_tutorial.ipynb). For an introduction to Multi-Task Learning (MTL), see [Multi-Task Tutorial](http://syndrome:8888/notebooks/snorkel-tutorials/multitask/multitask_tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snorkel Introduction\n",
    "\n",
    "from collections import OrderedDict \n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import random\n",
    "import snorkel\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "from spacy.pipeline import merge_entities\n",
    "\n",
    "# Add parent directory to path\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Make reproducible\n",
    "random.seed(1337)\n",
    "\n",
    "# Turn off TensorFlow logging messages\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# For reproducibility\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"1337\"\n",
    "\n",
    "# Show wide columns\n",
    "pd.set_option('display.max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_LIMIT = 50\n",
    "\n",
    "PATHS = {\n",
    "    'questions': {\n",
    "        'local': '../data/stackoverflow/Questions.Tags.{}.parquet',\n",
    "        'local_single': '../data/stackoverflow/Questions.Tags.{}.parquet/part-00029-1ad544ea-abd4-4960-aa2c-7e0eb12cdb8e-c000.snappy.parquet',\n",
    "        's3': 's3://stackoverflow-events/08-05-2019/Questions.Tags.{}.parquet',\n",
    "    },\n",
    "    'questions_parts_json': {\n",
    "        's3': 's3://stackoverflow-events/08-05-2019/Entity.Candidates.{}.jsonl',\n",
    "    },\n",
    "    'questions_parts_parquet': {\n",
    "        's3': 's3://stackoverflow-events/08-05-2019/Entity.Candidates.{}.parquet',\n",
    "    },\n",
    "    'entity_candidates': {\n",
    "        'local': '../data/stackoverflow/Entity.Candidates.{}.parquet',\n",
    "        'local_single': '../data/stackoverflow/Entity.Candidates.{}.parquet/part-00000*.parquet',\n",
    "        's3': 's3://stackoverflow-events/08-05-2019/Entity.Candidates.{}.parquet',\n",
    "    },\n",
    "    'gold_labels': {\n",
    "        's3': 's3://stackoverflow-events/text_extractions.one_file.df_out.gold.labeled.final.csv',\n",
    "        'local': '../data/text_extractions.one_file.df_out.gold.labeled.final.csv',\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define a set of paths for each step for local and S3\n",
    "PATH_SET = 's3' # 'local_single'\n",
    "\n",
    "path = PATHS['questions'][PATH_SET].format(TAG_LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our Examples for Augmentation in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_PostId</th>\n",
       "      <th>_AcceptedAnswerId</th>\n",
       "      <th>_Body</th>\n",
       "      <th>_Code</th>\n",
       "      <th>_Tags</th>\n",
       "      <th>_Label</th>\n",
       "      <th>_AnswerCount</th>\n",
       "      <th>_CommentCount</th>\n",
       "      <th>_FavoriteCount</th>\n",
       "      <th>_OwnerUserId</th>\n",
       "      <th>...</th>\n",
       "      <th>_AccountId</th>\n",
       "      <th>_UserId</th>\n",
       "      <th>_UserDisplayName</th>\n",
       "      <th>_UserDownVotes</th>\n",
       "      <th>_UserLocation</th>\n",
       "      <th>_ProfileImageUrl</th>\n",
       "      <th>_UserReputation</th>\n",
       "      <th>_UserUpVotes</th>\n",
       "      <th>_UserViews</th>\n",
       "      <th>_UserWebsiteUrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5883491</td>\n",
       "      <td>5883590.0</td>\n",
       "      <td>vb.net triple dot syntax for linq to xml I ran into this answer which had a triple dot syntax in VB.NET that I have never seen before.\\nThe query looks like this\\n\\nI tried to search on google about this triple dot syntax but I didn't get anything.\\nCan someone point to to some documentation abo...</td>\n",
       "      <td>Dim result =\\n   From xcmp In azm...&lt;Item&gt;.&lt;ItemPrice&gt;.&lt;Component&gt;\\n   Where xcmp.&lt;Type&gt;.Value = \"Principal\"\\n   Select Convert.ToDecimal(xcmp.&lt;Amount&gt;.Value)\\n</td>\n",
       "      <td>[c#, vb.net, syntax]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5883669</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Part of javascripts gets cut off in browser Greetings\\nOne of my clients javascript files is failing. I have found the reason, but the discovery has made me REALLY confused as I have never seen anything like it.\\nThe issue is that when the browser reads through the script sources and enters a sp...</td>\n",
       "      <td>if (isValidWidthChange) {\\nif (isValidWidthChan\\nif (isValidWidthChange) {\\nif (isValidWidthChan\\n(function($) {\\n    // jQuery autoGrowInput plugin by James Padolsey\\n    // See related thread: http://stackoverflow.com/questions/931207/is-there-a-jquery-autogrow-plugin-for-text-fields\\n    $.fn...</td>\n",
       "      <td>[javascript, jquery, sharepoint-2007]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5884365</td>\n",
       "      <td>5889005.0</td>\n",
       "      <td>install cocos2d for xcode4 I download cocos2d iphone 1.0.0 and cocos2d 1.0.0 beta template for xcode4 and i want use cocos2d in xcode4. \\nHow to install it ?\\n</td>\n",
       "      <td></td>\n",
       "      <td>[iphone, xcode4, cocos2d-iphone]</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5885914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>'pattern matching' on Typeable types Suppose, for example, we have the following data structure:\\n\\nNow, is there an easier way to do this:\\n\\nHas someone thought of making a syntax for pattern matching on Typeable types?\\n</td>\n",
       "      <td>data Foo = Bool Bool | Int Int | Double Double\\n\\nfoo :: Typeable a =&gt; a -&gt; Foo\\nfoo x = maybe (error \"i dunno\") id $\\n  liftM Bool   (cast x) `mplus`\\n  liftM Int    (cast x) `mplus`\\n  liftM Double (cast x)\\n</td>\n",
       "      <td>[haskell, syntax, types, dynamic-typing]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5886705</td>\n",
       "      <td>5886757.0</td>\n",
       "      <td>Haskell - Checking the Validity of a File Handle Ok, guys, super easy question (it seems weird that Google didn't help me with this one):\\n\\nHow do I check if the handle that I got from  is a valid handle, i.e. that the file exists and was opened successfully?\\n</td>\n",
       "      <td>import IO\\n\\n--.... yadda, yadda, yadda\\n\\n  file &lt;- openFile \"/some/path\" ReadMode\\n\\nopenFile</td>\n",
       "      <td>[exception, haskell, file-io, io, monads]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _PostId  _AcceptedAnswerId  \\\n",
       "0  5883491          5883590.0   \n",
       "1  5883669                NaN   \n",
       "2  5884365          5889005.0   \n",
       "3  5885914                NaN   \n",
       "4  5886705          5886757.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                         _Body  \\\n",
       "0  vb.net triple dot syntax for linq to xml I ran into this answer which had a triple dot syntax in VB.NET that I have never seen before.\\nThe query looks like this\\n\\nI tried to search on google about this triple dot syntax but I didn't get anything.\\nCan someone point to to some documentation abo...   \n",
       "1  Part of javascripts gets cut off in browser Greetings\\nOne of my clients javascript files is failing. I have found the reason, but the discovery has made me REALLY confused as I have never seen anything like it.\\nThe issue is that when the browser reads through the script sources and enters a sp...   \n",
       "2                                                                                                                                              install cocos2d for xcode4 I download cocos2d iphone 1.0.0 and cocos2d 1.0.0 beta template for xcode4 and i want use cocos2d in xcode4. \\nHow to install it ?\\n   \n",
       "3                                                                              'pattern matching' on Typeable types Suppose, for example, we have the following data structure:\\n\\nNow, is there an easier way to do this:\\n\\nHas someone thought of making a syntax for pattern matching on Typeable types?\\n   \n",
       "4                                       Haskell - Checking the Validity of a File Handle Ok, guys, super easy question (it seems weird that Google didn't help me with this one):\\n\\nHow do I check if the handle that I got from  is a valid handle, i.e. that the file exists and was opened successfully?\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                         _Code  \\\n",
       "0                                                                                                                                             Dim result =\\n   From xcmp In azm...<Item>.<ItemPrice>.<Component>\\n   Where xcmp.<Type>.Value = \"Principal\"\\n   Select Convert.ToDecimal(xcmp.<Amount>.Value)\\n   \n",
       "1  if (isValidWidthChange) {\\nif (isValidWidthChan\\nif (isValidWidthChange) {\\nif (isValidWidthChan\\n(function($) {\\n    // jQuery autoGrowInput plugin by James Padolsey\\n    // See related thread: http://stackoverflow.com/questions/931207/is-there-a-jquery-autogrow-plugin-for-text-fields\\n    $.fn...   \n",
       "2                                                                                                                                                                                                                                                                                                                \n",
       "3                                                                                           data Foo = Bool Bool | Int Int | Double Double\\n\\nfoo :: Typeable a => a -> Foo\\nfoo x = maybe (error \"i dunno\") id $\\n  liftM Bool   (cast x) `mplus`\\n  liftM Int    (cast x) `mplus`\\n  liftM Double (cast x)\\n   \n",
       "4                                                                                                                                                                                                              import IO\\n\\n--.... yadda, yadda, yadda\\n\\n  file <- openFile \"/some/path\" ReadMode\\n\\nopenFile   \n",
       "\n",
       "                                       _Tags  _Label  _AnswerCount  \\\n",
       "0                       [c#, vb.net, syntax]       0             3   \n",
       "1      [javascript, jquery, sharepoint-2007]       0             2   \n",
       "2           [iphone, xcode4, cocos2d-iphone]       0             5   \n",
       "3   [haskell, syntax, types, dynamic-typing]       0             3   \n",
       "4  [exception, haskell, file-io, io, monads]       0             1   \n",
       "\n",
       "   _CommentCount  _FavoriteCount  _OwnerUserId  ... _AccountId  _UserId  \\\n",
       "0              0             1.0           NaN  ...        NaN      NaN   \n",
       "1              5             NaN           NaN  ...        NaN      NaN   \n",
       "2              0             1.0           NaN  ...        NaN      NaN   \n",
       "3              0             0.0           NaN  ...        NaN      NaN   \n",
       "4              1             NaN           NaN  ...        NaN      NaN   \n",
       "\n",
       "   _UserDisplayName _UserDownVotes  _UserLocation  _ProfileImageUrl  \\\n",
       "0              None            NaN           None              None   \n",
       "1              None            NaN           None              None   \n",
       "2              None            NaN           None              None   \n",
       "3              None            NaN           None              None   \n",
       "4              None            NaN           None              None   \n",
       "\n",
       "  _UserReputation  _UserUpVotes _UserViews _UserWebsiteUrl  \n",
       "0             NaN           NaN        NaN            None  \n",
       "1             NaN           NaN        NaN            None  \n",
       "2             NaN           NaN        NaN            None  \n",
       "3             NaN           NaN        NaN            None  \n",
       "4             NaN           NaN        NaN            None  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\n",
    "    path, \n",
    "    engine='pyarrow',\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our Examples for Augmentation in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"Programming Language Extraction Example\") \\\n",
    "#     .getOrCreate()\n",
    "# sc = spark.sparkContext\n",
    "\n",
    "question_df = spark.read.parquet(path)\n",
    "# question_df.limit(3).toPandas()\n",
    "question_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable spaCy GPU Support\n",
    "\n",
    "That is, if you have a GPU and are using Pandas. `multiprocessing.Pool` Pandas and PySpark can't use a GPU yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a spaCy [`Language`](https://spacy.io/api/language) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "# Download the spaCy english model\n",
    "spacy.cli.download('en_core_web_lg')\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=[\"vectors\"])\n",
    "\n",
    "# Merge multi-token entities together\n",
    "nlp.add_pipe(merge_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f0da9e033d0>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f0da9eb6fa0>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f0da9eb6ec0>),\n",
       " ('merge_entities', <function spacy.pipeline.functions.merge_entities(doc)>)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring spaCy\n",
    "\n",
    "Below we use print statements and the visualization tool  `spacy.displacy` to render parsed objects for an example document. First we iterate the spaCy  [`Token`s](https://spacy.io/api/token) that make up the [`Doc`](https://spacy.io/api/doc) and print the text and the string defining their part-of-speech. \n",
    "\n",
    "We’ll be using these parts of speech to write Labeling Functions using spaCy pattern matching. It will be very useful to know the part of speech we’re looking for in our entities - almost exclusively proper nouns - `PROPN` - and quite often of the pattern `VERB-ADP-PROPN`, which we’ll see below.\n",
    "\n",
    "Next we use `spacy.displacy` to visualize the parse tree of dependencies between words as well as the entities detected in the sentence. This gives a rough idea of the structure that a spaCy `Doc` has for us to use. It also creates dense vectors based on embeddings for the words and the entire document, which we can use to create LFs as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('program', 'NOUN'), ('to', 'PART'), ('do', 'AUX'), ('payroll', 'NOUN'), ('was', 'AUX'), ('written', 'VERB'), ('in', 'ADP'), ('C++', 'PROPN'), ('and', 'CCONJ'), ('Perl', 'PROPN'), ('.', 'PUNCT')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"ba51c0e4d74749d38056998d8d2cd20d-0\" class=\"displacy\" width=\"1040\" height=\"272.0\" direction=\"ltr\" style=\"max-width: none; height: 272.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">program</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">do</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">payroll</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">written</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">C++</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">Perl.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ba51c0e4d74749d38056998d8d2cd20d-0-0\" stroke-width=\"2px\" d=\"M62,137.0 62,122.0 134.0,122.0 134.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ba51c0e4d74749d38056998d8d2cd20d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M62,139.0 L58,131.0 66,131.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ba51c0e4d74749d38056998d8d2cd20d-0-1\" stroke-width=\"2px\" d=\"M152,137.0 152,92.0 590.0,92.0 590.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ba51c0e4d74749d38056998d8d2cd20d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M152,139.0 L148,131.0 156,131.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ba51c0e4d74749d38056998d8d2cd20d-0-2\" stroke-width=\"2px\" d=\"M242,137.0 242,122.0 314.0,122.0 314.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ba51c0e4d74749d38056998d8d2cd20d-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M242,139.0 L238,131.0 246,131.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ba51c0e4d74749d38056998d8d2cd20d-0-3\" stroke-width=\"2px\" d=\"M152,137.0 152,107.0 317.0,107.0 317.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ba51c0e4d74749d38056998d8d2cd20d-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M317.0,139.0 L321.0,131.0 313.0,131.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ba51c0e4d74749d38056998d8d2cd20d-0-4\" stroke-width=\"2px\" d=\"M332,137.0 332,122.0 404.0,122.0 404.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ba51c0e4d74749d38056998d8d2cd20d-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M404.0,139.0 L408.0,131.0 400.0,131.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ba51c0e4d74749d38056998d8d2cd20d-0-5\" stroke-width=\"2px\" d=\"M512,137.0 512,122.0 584.0,122.0 584.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ba51c0e4d74749d38056998d8d2cd20d-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M512,139.0 L508,131.0 516,131.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ba51c0e4d74749d38056998d8d2cd20d-0-6\" stroke-width=\"2px\" d=\"M602,137.0 602,122.0 674.0,122.0 674.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ba51c0e4d74749d38056998d8d2cd20d-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M674.0,139.0 L678.0,131.0 670.0,131.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ba51c0e4d74749d38056998d8d2cd20d-0-7\" stroke-width=\"2px\" d=\"M692,137.0 692,122.0 764.0,122.0 764.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ba51c0e4d74749d38056998d8d2cd20d-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M764.0,139.0 L768.0,131.0 760.0,131.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ba51c0e4d74749d38056998d8d2cd20d-0-8\" stroke-width=\"2px\" d=\"M782,137.0 782,122.0 854.0,122.0 854.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ba51c0e4d74749d38056998d8d2cd20d-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M854.0,139.0 L858.0,131.0 850.0,131.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ba51c0e4d74749d38056998d8d2cd20d-0-9\" stroke-width=\"2px\" d=\"M782,137.0 782,107.0 947.0,107.0 947.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ba51c0e4d74749d38056998d8d2cd20d-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M947.0,139.0 L951.0,131.0 943.0,131.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The program to do payroll was written in \n",
       "<mark class=\"entity\" style=\"background: #ff8197; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    C++\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LANGUAGE</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Perl\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "s = 'The program to do payroll was written in C++ and Perl.'\n",
    "d = nlp(s)\n",
    "tups = []\n",
    "for t in d:\n",
    "    tups.append((t.text, t.pos_))\n",
    "\n",
    "# Print words/parts-of-speech\n",
    "print([x for x in tups])\n",
    "\n",
    "# Render image diagrams\n",
    "displacy.render(d, style='dep', options={'compact': True, 'collapse_punct': True, 'distance': 90}, )\n",
    "displacy.render(d, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Pandas, produce records with their left/right tokens for all entities in all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Pandas - single processor\n",
    "#\n",
    "window = 4\n",
    "candidates = []\n",
    "for index, row in df.iterrows():\n",
    "    doc = nlp(row['_Body'])\n",
    "    re_doc_1 = nlp(row['body'])\n",
    "    re_doc_2 = nlp(row['body'])\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        rec = {}\n",
    "        rec['body'] = doc.text\n",
    "\n",
    "        rec['entity_text'] = ent.text\n",
    "        rec['entity_start'] = ent.start\n",
    "        rec['entity_end'] = ent.end\n",
    "        rec['ent_type'] = ent.label_\n",
    "\n",
    "        # Retokenize the tokens to the left of the entity into a single string\n",
    "        # using a new Doc without messing up the original Doc\n",
    "        left_token_start = max(0, ent.start - 1 - window)\n",
    "        left_token_end = ent.start\n",
    "        rec['left_tokens_text'] = [x.text for x in doc[left_token_start : left_token_end]]\n",
    "\n",
    "        left_merged_token = re_doc_1[left_token_start: left_token_end].merge()\n",
    "        rec['left_text'] = left_merged_token.text if left_merged_token else ''\n",
    "        del left_token_start\n",
    "        del left_token_end\n",
    "        del left_merged_token\n",
    "\n",
    "        # Retokenize the tokens to the left of the entity into a single string\n",
    "        # using a new Doc without messing up the original Doc\n",
    "        right_token_start = min(ent.end, len(doc) - 1)\n",
    "        right_token_end = min(ent.end + window, len(doc) - 1)\n",
    "        rec['right_tokens_text'] = [x.text for x in doc[right_token_start : right_token_end]]\n",
    "\n",
    "        right_merged_token = re_doc_2[right_token_start: right_token_end].merge()\n",
    "        rec['right_text'] = right_merged_token.text if right_merged_token else ''\n",
    "        del right_token_start\n",
    "        del right_token_end\n",
    "        del right_merged_token\n",
    "\n",
    "        rec['wikidata_id'] = ent.kb_id\n",
    "        \n",
    "        rec['original_index'] = index\n",
    "        rec['label'] = 0\n",
    "\n",
    "        candidates.append(rec)\n",
    "        del ent\n",
    "    \n",
    "    del doc\n",
    "    del re_doc_1\n",
    "    del re_doc_2\n",
    "    gc.collect()\n",
    "\n",
    "df_out = pd.DataFrame(candidates)\n",
    "df_out = df_out.reindex().sort_index()\n",
    "\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `multiprocessing.Pool` to Speed up Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_split(df: pd.DataFrame, window: int=5):\n",
    "\n",
    "    candidates = []\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        # Need 3 docs since retokenizing the left/right tokens shortens the Doc, \n",
    "        # messing up our start/stop indexes and Docs can't be copied.\n",
    "        doc = nlp(row['_Body'])\n",
    "        re_doc_1 = nlp(row['_Body'])\n",
    "        re_doc_2 = nlp(row['_Body'])\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            rec = {}\n",
    "            rec['body'] = doc.text\n",
    "\n",
    "            rec['entity_text'] = ent.text\n",
    "            rec['entity_start'] = ent.start\n",
    "            rec['entity_end'] = ent.end\n",
    "            rec['ent_type'] = ent.label_\n",
    "\n",
    "            # Retokenize the tokens to the left of the entity into a single string\n",
    "            # using a new Doc without messing up the original Doc\n",
    "            left_token_start = max(0, ent.start - 1 - window)\n",
    "            left_token_end = ent.start\n",
    "            rec['left_tokens_text'] = [x.text for x in doc[left_token_start : left_token_end]]\n",
    "            \n",
    "            left_merged_token = re_doc_1[left_token_start: left_token_end].merge()\n",
    "            rec['left_text'] = left_merged_token.text if left_merged_token else ''\n",
    "            del left_token_start\n",
    "            del left_token_end\n",
    "            del left_merged_token\n",
    "            \n",
    "            # Retokenize the tokens to the left of the entity into a single string\n",
    "            # using a new Doc without messing up the original Doc\n",
    "            right_token_start = min(ent.end, len(doc) - 1)\n",
    "            right_token_end = min(ent.end + window, len(doc) - 1)\n",
    "            rec['right_tokens_text'] = [x.text for x in doc[right_token_start : right_token_end]]\n",
    "            \n",
    "            right_merged_token = re_doc_2[right_token_start: right_token_end].merge()\n",
    "            rec['right_text'] = right_merged_token.text if right_merged_token else ''\n",
    "            del right_token_start\n",
    "            del right_token_end\n",
    "            del right_merged_token\n",
    "\n",
    "            rec['wikidata_id'] = ent.kb_id\n",
    "\n",
    "            rec['original_index'] = index\n",
    "            rec['label'] = 0\n",
    "\n",
    "            candidates.append(rec)\n",
    "            del ent\n",
    "            \n",
    "        del doc\n",
    "        del re_doc_1\n",
    "        del re_doc_2\n",
    "        gc.collect()\n",
    "\n",
    "    df_out = pd.DataFrame(candidates)\n",
    "    df_out = df_out.reindex().sort_index()\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count, Pool\n",
    "\n",
    "\n",
    "def restore_spacy(df, n_cores=cpu_count):\n",
    "    \n",
    "    n_cores = cpu_count() if callable(cpu_count) else cpu_count\n",
    "    \n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    \n",
    "    df_out = pd.concat(\n",
    "        pool.map(\n",
    "            process_split,\n",
    "            df_split\n",
    "        )\n",
    "    )\n",
    "    df_out = df_out.reindex().sort_index()\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "df_out = restore_spacy(df)\n",
    "\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_path = PATHS['entity_candidates'][PATH_SET].format(TAG_LIMIT)\n",
    "\n",
    "df_out.to_parquet(\n",
    "    entity_path,\n",
    "    engine='pyarrow'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In PySpark, produce records with their left/right tokens for all entities in all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def prepare_docs(rows: Any, window: int=5):\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_lg\", disable=[\"vectors\"])\n",
    "    nlp.add_pipe(merge_entities)\n",
    "    \n",
    "    recs = []\n",
    "    \n",
    "    for row in rows:\n",
    "        \n",
    "        # Need 3 docs since retokenizing the left/right tokens shortens the Doc, \n",
    "        # messing up our start/stop indexes\n",
    "        doc = nlp(row._Body)\n",
    "        re_doc_1 = nlp(row._Body)\n",
    "        re_doc_2 = nlp(row._Body)\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            rec = {}\n",
    "            rec['body'] = doc.text\n",
    "\n",
    "            rec['entity_text'] = ent.text\n",
    "            rec['entity_start'] = ent.start\n",
    "            rec['entity_end'] = ent.end\n",
    "            rec['ent_type'] = ent.label_\n",
    "            \n",
    "            # Retokenize the tokens to the left of the entity into a single string\n",
    "            # using a new Doc without messing up the original Doc\n",
    "            left_token_start = max(0, ent.start - 1 - window)\n",
    "            left_token_end = ent.start\n",
    "            rec['left_tokens_text'] = [x.text for x in doc[left_token_start : left_token_end]]\n",
    "\n",
    "            left_merged_token = re_doc_1[left_token_start: left_token_end].merge()\n",
    "            rec['left_text'] = left_merged_token.text if left_merged_token else ''\n",
    "            del left_token_start\n",
    "            del left_token_end\n",
    "            del left_merged_token\n",
    "            \n",
    "            # Retokenize the tokens to the left of the entity into a single string\n",
    "            # using a new Doc without messing up the original Doc\n",
    "            right_token_start = min(ent.end, len(doc) - 1)\n",
    "            right_token_end = min(ent.end + window, len(doc) - 1)\n",
    "            rec['right_tokens_text'] = [x.text for x in doc[right_token_start : right_token_end]]\n",
    "            \n",
    "            right_merged_token = re_doc_2[right_token_start: right_token_end].merge()\n",
    "            rec['right_text'] = right_merged_token.text if right_merged_token else ''\n",
    "            del right_token_start\n",
    "            del right_token_end\n",
    "            del right_merged_token\n",
    "            \n",
    "            rec['wikidata_id'] = ent.kb_id\n",
    "            rec['label'] = 0\n",
    "            \n",
    "            recs.append(rec)\n",
    "            del ent\n",
    "            \n",
    "        del doc\n",
    "        del re_doc_1\n",
    "        del re_doc_2\n",
    "        gc.collect()\n",
    "    \n",
    "    return recs\n",
    "\n",
    "# Repartition the data to work with many mappers\n",
    "question_partitioned_df = question_df.repartition(500, F.col('_PostId'))\n",
    "entity_rdd = question_partitioned_df.rdd.mapPartitions(prepare_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data as JSON Lines\n",
    "entity_json_path = PATHS['questions_parts_json'][PATH_SET].format(TAG_LIMIT)\n",
    "entity_rdd.map(lambda x: json.dumps(x)).saveAsTextFile(entity_json_path)\n",
    "\n",
    "# Load as JSON Lines into a DataFrame then store as Parquet (way faster than converting the RDD to DataFrame)\n",
    "entity_df = spark.read.json(entity_json_path)\n",
    "entity_parquet_path = PATHS['questions_parts_parquet'][PATH_SET].format(TAG_LIMIT)\n",
    "entity_df.write.mode('overwrite').parquet(entity_parquet_path)\n",
    "entity_df = spark.read.parquet(entity_parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Gold Labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "gold_path = PATHS['gold_labels'][PATH_SET].format(TAG_LIMIT)\n",
    "df_gold = pd.read_csv(gold_path)\n",
    "\n",
    "# Drop the index column, we have an index set\n",
    "df_gold = df_gold.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "df_gold['left_tokens_text'] = df_gold['left_tokens_text'].apply(lambda x: ast.literal_eval(x))\n",
    "df_gold['right_tokens_text'] = df_gold['right_tokens_text'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "df_gold.tail()\n",
    "\n",
    "# Start the rest of the data after the point where the labeled data starts\n",
    "df_in = df_out.iloc[df_gold.index[-1] + 1:, :]\n",
    "df_in.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Split Data into Train / Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test, y_train, y_test = train_test_split(\n",
    "    df_in_fixed, \n",
    "    df_in_fixed['label'].values, \n",
    "    test_size=0.3,\n",
    "    random_state=1337,\n",
    ")\n",
    "\n",
    "len(df_train.index), len(df_test.index), y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Split Data into Train / Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for PySpark\n",
    "df_train, df_test = entity_df.randomSplit([0.7, 0.3], seed=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Labels for Language Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup our spaCy Preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jsonlines, sys\n",
    "from snorkel.labeling import labeling_function, LabelingFunction\n",
    "from snorkel.preprocess import preprocessor\n",
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "spacy = SpacyPreprocessor(\n",
    "    text_field='body',\n",
    "    doc_field='spacy',\n",
    "    memoize=True,\n",
    "    language='en_core_web_lg',\n",
    "    disable=['vectors'],\n",
    ")\n",
    "\n",
    "@preprocessor(memoize=True, pre=[spacy])\n",
    "def restore_entity(x):\n",
    "    \n",
    "    entity = None\n",
    "    for ent in x['spacy'].ents:\n",
    "        if  ent.start == row['entity_start'] \\\n",
    "        and ent.end   == row['entity_end']:\n",
    "            entity = ent\n",
    "\n",
    "    if entity is None:\n",
    "        raise Exception('Missing entity!')\n",
    "\n",
    "    x['entity'] = entity\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic Functions\n",
    "\n",
    "These Labeling Functiuons are rules based on clues that come from inspecting the data through exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts_rx = re.compile('^\\W')\n",
    "          \n",
    "@labeling_function()\n",
    "def lf_starts_with_char(x):\n",
    "    \"\"\"NEGATIVE if starts with a non-alpha-numeric value\"\"\"\n",
    "    return NEGATIVE if starts_rx.match(x['entity_text']) else ABSTAIN\n",
    "\n",
    "\n",
    "number_end_rx = re.compile('^[a-zA-Z]+[0-9\\W]+$')\n",
    "\n",
    "@labeling_function()\n",
    "def lf_ends_with_symbol_or_number(x):\n",
    "    \"\"\"POSITIVE if starts with letter and ends in number\"\"\"\n",
    "    return POSITIVE if number_end_rx.match(x['entity_text']) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_wrong_entity_type(x):\n",
    "    return NEGATIVE if x['ent_type'] in ['PERSON', 'NORP', 'FAC', 'GPE', 'LOC', \n",
    "                                         'LAW', 'DATE', 'TIME', 'PERCENT',\n",
    "                                         'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL',] else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_token_count_2(x):\n",
    "    \"\"\"NEGATIVE if entity has more than 2 words in it\"\"\"\n",
    "    return NEGATIVE if len(x['entity_text'].split(' ')) > 2 else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_token_count_1(x):\n",
    "    \"\"\"NEGATIVE if entity has more than 1 word in it\"\"\"\n",
    "    return NEGATIVE if len(x['entity_text'].split(' ')) > 1 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Lookups\n",
    "\n",
    "These are the simplest type of Labeling Functions that use the presence of one or more words in a document to classify it as `POSITIVE` or `NEGATIVE` or to `ABSTAIN` if it is not sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Make keyword LF generation\n",
    "#\n",
    "def keyword_lookup(x, keywords, field, label):\n",
    "    \"\"\"Perform lowercase matching for keyword LFs\"\"\"\n",
    "    print(type(x[field]), x[field], x)\n",
    "    match = any(word.lower() in x[field].lower() for word in keywords)\n",
    "    if match:\n",
    "        return label\n",
    "    return ABSTAIN\n",
    "\n",
    "def make_keyword_lf(keywords, field='body', label=ABSTAIN):\n",
    "    \"\"\"Given keywords, a field to match against and a label to return, return an keyword LF\"\"\"\n",
    "    return LabelingFunction(\n",
    "        name=f\"keyword_{keywords}\",\n",
    "        f=keyword_lookup,\n",
    "        resources=dict(keywords=keywords, field=field, label=label),\n",
    "    )\n",
    "\n",
    "# Define keyword LFs\n",
    "lf_language_keyword = make_keyword_lf(['language'], 'left_text', label=POSITIVE)\n",
    "lf_written_keyword = make_keyword_lf(['written'], 'left_text', label=POSITIVE)\n",
    "lf_framework_keyword = make_keyword_lf(['framework', 'package'], 'right_text', label=NEGATIVE)\n",
    "\n",
    "#\n",
    "# Use regular expressions to negate browsers\n",
    "#\n",
    "prefixes = ['internet', 'ie', 'firefox', 'google', 'chrome', 'apple', 'safari', 'webkit', 'gecko', \n",
    "            'opera', 'netscape', 'chromium', ]\n",
    "browser_rx = re.compile(''.join(['^(?:', '|'.join(prefixes), ')']))\n",
    "\n",
    "@labeling_function()\n",
    "def lf_not_browser(x):\n",
    "    \"\"\"Eliminate browser false positives\"\"\"\n",
    "    e = x['entity_text'].lower()\n",
    "    return NEGATIVE if browser_rx.match(e) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distant Supervision\n",
    "\n",
    "In these Labeling Functiuns, we use clues from external sources of data such as WikiData to classify documents. For example, we use WikiData's list of programming languages to determine whether a token is a programming language. Likewise we use the presence of part of an operating system from WikiData's name to classify documents as `NEGATIVE`, since operating systems are not progrmaming languages :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import boto3\n",
    "\n",
    "def install(x):\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    if os.path.isfile('/tmp/programming_languages.jsonl'):\n",
    "        pass\n",
    "    else:\n",
    "        s3.download_file('stackoverflow-events', 'programming_languages.jsonl', '/tmp/programming_languages.jsonl')\n",
    "\n",
    "    if os.path.isfile('/tmp/operating_systems.jsonl'):\n",
    "        pass\n",
    "    else:\n",
    "        s3.download_file('stackoverflow-events', 'operating_systems.jsonl', '/tmp/operating_systems.jsonl')\n",
    "\n",
    "    return [1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Label functions using distant supervision from SPARQL/WikiData for programming languages\n",
    "#\n",
    "languages, lower_languages = None, None\n",
    "with jsonlines.open('/tmp/programming_languages.jsonl', mode='r') as reader:\n",
    "    languages = [x['name'] for x in reader]\n",
    "    lower_languages = [x.lower() for x in languages]\n",
    "\n",
    "@labeling_function(resources=dict(languages=languages))\n",
    "def lf_matches_wikidata_langs(x, languages):\n",
    "    \"\"\"POSITIVE if the entity_text matches any language in list\"\"\"\n",
    "    return POSITIVE if x['entity_text'] in languages else ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(lower_languages=lower_languages))\n",
    "def lf_lower_matches_wikidata_langs(x, lower_languages):\n",
    "    \"\"\"POSITIVE if the lowercase entity_text matches any lowercase language in list\"\"\"\n",
    "    return POSITIVE if x['entity_text'].lower() in lower_languages else ABSTAIN\n",
    "\n",
    "# Label functions using distant supervision from SPARQL/WikiData for operating systems\n",
    "oses, os_parts = [], []\n",
    "with jsonlines.open('/tmp/operating_systems.jsonl', mode='r') as reader:\n",
    "    oses = [x['name'].lower() for x in reader]\n",
    "    for os in oses:\n",
    "        for os_part in os.split():\n",
    "            os_parts.append(os_part)\n",
    "\n",
    "@labeling_function(resources=dict(oses=oses))\n",
    "def lf_matches_wikidata_os(x, oses):\n",
    "    \"\"\"NEGATIVE if the lowercase entity_text matches any lowercase OS in the list\"\"\"\n",
    "    return NEGATIVE if x['entity_text'].lower() in oses else ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(os_parts=os_parts))\n",
    "def lf_matches_wikidata_os_parts(x, os_parts):\n",
    "    \"\"\"NEGATIVE if the lowercase entity_text matches any lowercase OS fragment in the list\"\"\"\n",
    "    return NEGATIVE if x['entity_text'].lower() in os_parts else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy Pattern Matching Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'POS': 'VERB'}, {'POS': 'ADP'}, {'POS': 'PROPN'}]\n",
    "matcher.add(\"VERB_ADP_PROPN\", None, pattern)\n",
    "\n",
    "@labeling_function(pre=[spacy, restore_entity])\n",
    "def lf_verb_in_noun(x):\n",
    "    \"\"\"Return positive if the pattern\"\"\"\n",
    "    sp = x['spacy']\n",
    "    matches = matcher(sp)\n",
    "    \n",
    "    found = False\n",
    "    for match_id, start, end in matches:\n",
    "        if end == x['entity_end']:\n",
    "            pass\n",
    "        if start == x['start'] - 2:            \n",
    "            if sp[start].text in ['work', 'written', 'wrote']:                \n",
    "                if sp[start + 1].text in ['in']:\n",
    "                    return POSITIVE\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Model Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define What LFs will Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristic_lfs = [\n",
    "    lf_starts_with_char,\n",
    "    lf_ends_with_symbol_or_number,\n",
    "    lf_wrong_entity_type,\n",
    "    lf_token_count_1,\n",
    "    lf_token_count_2,\n",
    "]\n",
    "\n",
    "keyword_lfs = [\n",
    "    lf_language_keyword,\n",
    "    lf_written_keyword,\n",
    "    lf_framework_keyword,\n",
    "    lf_not_browser,\n",
    "]\n",
    "\n",
    "distant_lfs = [\n",
    "    lf_matches_wikidata_langs,\n",
    "    lf_lower_matches_wikidata_langs,\n",
    "    lf_matches_wikidata_os,\n",
    "    lf_matches_wikidata_os_parts,\n",
    "]\n",
    "\n",
    "# pattern_match_lfs = [\n",
    "#     lf_verb_in_noun,\n",
    "# ]\n",
    "\n",
    "lfs = heuristic_lfs + keyword_lfs + distant_lfs # + pattern_match_lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test LF Accuracy on Gold Labeled Data\n",
    "\n",
    "We need to see how well the LFs work in classifying data with known labels, our gold labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFAnalysis, PandasLFApplier\n",
    "\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_dev = applier.apply(df_gold)\n",
    "y_dev = df_gold.label.values\n",
    "LFAnalysis(L_dev, lfs).lf_summary(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.apply.spark import SparkLFApplier\n",
    "\n",
    "# Get dicts that we can work with in LFs\n",
    "entity_rdd = entity_df.rdd.repartition(16 * 40).map(lambda x: x.asDict())\n",
    "\n",
    "spark_applier = SparkLFApplier(lfs)\n",
    "L_train = spark_applier.apply(entity_rdd)\n",
    "\n",
    "LFAnalysis(L_train, lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze and Improve the Performance of our LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(L_train != ABSTAIN).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.analysis import get_label_buckets\n",
    "\n",
    "buckets = get_label_buckets(y_dev, L_dev[:, 1])\n",
    "\n",
    "df_gold.iloc[buckets[NEGATIVE, POSITIVE]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit our `LabelModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, None, n_epochs=5000, log_freq=500, seed=1337)\n",
    "\n",
    "label_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "probs_dev = label_model.predict_proba(L_dev)\n",
    "preds_dev = probs_to_preds(probs_dev)\n",
    "print(\n",
    "    f\"Label model accuracy score: {metric_score(y_dev, preds_dev, probs=probs_dev, metric='accuracy')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Label model precision score: {metric_score(y_dev, preds_dev, probs=probs_dev, metric='precision')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Label model recall score: {metric_score(y_dev, preds_dev, probs=probs_dev, metric='recall')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Label model f1 score: {metric_score(y_dev, preds_dev, probs=probs_dev, metric='f1')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Label model roc-auc: {metric_score(y_dev, preds_dev, probs=probs_dev, metric='roc_auc')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out Unlabeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "probs_train = label_model.predict_proba(L_train)\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\n",
    "    X=df_train, y=probs_train, L=L_train\n",
    ")\n",
    "\n",
    "df_train_filtered.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our Discriminative Bidirectional LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this model is pulled from the Snorkel Spouse Example: https://www.snorkel.org/use-cases/spouse-demo\n",
    "\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Bidirectional,\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    Input,\n",
    "    LSTM,\n",
    ")\n",
    "\n",
    "\n",
    "# elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)\n",
    "\n",
    "\n",
    "def get_feature_arrays(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Get np arrays of upto max_length tokens and person idxs.\"\"\"\n",
    "    left = df['left_tokens_text']\n",
    "    right = df['right_tokens_text']\n",
    "\n",
    "    def pad_or_truncate(l, max_length=40):\n",
    "        pad_length = max_length - len(l)\n",
    "        padding = [\"\"] * pad_length\n",
    "        left_values = l[:max_length]\n",
    "        padded_values = np.append(left_values, padding)\n",
    "        return padded_values\n",
    "\n",
    "    left_tokens = np.array(list(map(pad_or_truncate, left)))\n",
    "    right_tokens = np.array(list(map(pad_or_truncate, right)))\n",
    "    return left_tokens, right_tokens\n",
    "\n",
    "\n",
    "def bilstm(\n",
    "    tokens: tf.Tensor,\n",
    "    rnn_state_size: int = 64,\n",
    "    num_buckets: int = 40000,\n",
    "    embed_dim: int = 36,\n",
    "):\n",
    "    ids = tf.strings.to_hash_bucket(tokens, num_buckets)\n",
    "    embedded_input = Embedding(num_buckets, embed_dim)(ids)\n",
    "    return Bidirectional(LSTM(rnn_state_size, activation=tf.nn.relu))(\n",
    "        embedded_input, mask=tf.strings.length(tokens)\n",
    "    )\n",
    "\n",
    "\n",
    "# def char_emb(\n",
    "#     tokens: tf.Tensor,\n",
    "# ):\n",
    "#     embeddings = elmo(\n",
    "#     [\"the cat is on the mat\", \"dogs are in the fog\"],\n",
    "#     signature=\"default\",\n",
    "#     as_dict=True)[\"elmo\"]\n",
    "\n",
    "\n",
    "def get_model(\n",
    "    rnn_state_size: int = 64, num_buckets: int = 40000, embed_dim: int = 12\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Return LSTM model for predicting label probabilities.\n",
    "    Args:\n",
    "        rnn_state_size: LSTM state size.\n",
    "        num_buckets: Number of buckets to hash strings to integers.\n",
    "        embed_dim: Size of token embeddings.\n",
    "    Returns:\n",
    "        model: A compiled LSTM model.\n",
    "    \"\"\"\n",
    "    left_ph = Input((None,), dtype=\"string\")\n",
    "    # entity_ph = Input((None,), dtype=\"string\")\n",
    "    right_ph = Input((None,), dtype=\"string\")\n",
    "    left_embs = bilstm(left_ph, rnn_state_size, num_buckets, embed_dim)\n",
    "    # char_embs = char_emb()\n",
    "    right_embs = bilstm(right_ph, rnn_state_size, num_buckets, embed_dim)\n",
    "    layer = Concatenate(1)([left_embs, right_embs])\n",
    "    layer = Dense(64, activation=tf.nn.relu)(layer)\n",
    "    layer = Dense(32, activation=tf.nn.relu)(layer)\n",
    "    probabilities = Dense(2, activation=tf.nn.softmax)(layer)\n",
    "    model = tf.keras.Model(inputs=[left_ph, right_ph], outputs=probabilities)\n",
    "    model.compile(tf.compat.v1.train.AdagradOptimizer(0.1), \"categorical_crossentropy\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = get_feature_arrays(df_train_filtered)\n",
    "model = get_model()\n",
    "batch_size = 64\n",
    "model.fit(X_train, probs_train_filtered, batch_size=batch_size, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalute Discriminative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gold = get_feature_arrays(df_gold)\n",
    "probs_gold = model.predict(X_gold)\n",
    "preds_gold = probs_to_preds(probs_gold)\n",
    "\n",
    "print(\n",
    "    f\"Gold accuracy score                       : {metric_score(y_dev, preds=preds_gold, probs=probs_gold, metric='accuracy')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Gold precision score                      : {metric_score(y_dev, preds=preds_gold, probs=probs_gold, metric='precision')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Gold recall score                         : {metric_score(y_dev, preds=preds_gold, probs=probs_gold, metric='recall')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Gold F1 when trained with hard labels     : {metric_score(y_dev, preds=preds_gold, metric='f1')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Gold ROC-AUC when trained with soft labels: {metric_score(y_dev, probs=probs_gold, metric='roc_auc')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gold['pred_prob'] = probs_gold[:,1]\n",
    "df_gold['pred_label'] = preds_gold\n",
    "df_gold.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_feature_arrays(df_test.head(3))\n",
    "df_test['left_tokens_text'].iloc[0]\n",
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
