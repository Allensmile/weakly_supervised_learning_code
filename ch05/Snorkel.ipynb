{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Snorkel\n",
    "\n",
    "In this notebook we will use Snorkel to enrich our data such that tags with between 500-2,000 examples will be labeled using weak supervision to produce labels for enough examples to allow us to train an accurate full model that includes these new labels.\n",
    "\n",
    "More information about Snorkel can be found at [Snorkel.org](https://www.snorkel.org/) :) For a basic introduction to Snorkel, see the [Spam Tutorial](http://syndrome:8888/notebooks/snorkel-tutorials/spam/01_spam_tutorial.ipynb). For an introduction to Multi-Task Learning (MTL), see [Multi-Task Tutorial](http://syndrome:8888/notebooks/snorkel-tutorials/multitask/multitask_tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snorkel Introduction\n",
    "\n",
    "from collections import OrderedDict \n",
    "from glob import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import cupy\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import random\n",
    "import snorkel\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "\n",
    "# Add parent directory to path\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Make reproducible\n",
    "random.seed(1337)\n",
    "\n",
    "Turn off TensorFlow logging messages\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# For reproducibility\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"1337\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_LIMIT = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS = {\n",
    "    'questions': {\n",
    "        'local': '../data/stackoverflow/Questions.Tags.{}.parquet/part-00000-93547d3c-1f08-40b5-923e-89b961d01fc2-c000.snappy.parquet',\n",
    "        's3': 's3://stackoverflow-events/08-05-2019/Questions.Tags.{}.parquet/part-00000-93547d3c-1f08-40b5-923e-89b961d01fc2-c000.snappy.parquet',\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define a set of paths for each step for local and S3\n",
    "PATH_SET = 's3' # 'local'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our Examples for Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = PATHS['questions'][PATH_SET].format(TAG_LIMIT)#, BAD_LIMIT)\n",
    "\n",
    "df = pd.read_parquet(\n",
    "    path, \n",
    "    engine='pyarrow',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_PostId</th>\n",
       "      <th>_AcceptedAnswerId</th>\n",
       "      <th>_Body</th>\n",
       "      <th>_Code</th>\n",
       "      <th>_Tags</th>\n",
       "      <th>_AnswerCount</th>\n",
       "      <th>_CommentCount</th>\n",
       "      <th>_FavoriteCount</th>\n",
       "      <th>_OwnerUserId</th>\n",
       "      <th>_OwnerDisplayName</th>\n",
       "      <th>...</th>\n",
       "      <th>_AccountId</th>\n",
       "      <th>_UserId</th>\n",
       "      <th>_UserDisplayName</th>\n",
       "      <th>_UserDownVotes</th>\n",
       "      <th>_UserLocation</th>\n",
       "      <th>_ProfileImageUrl</th>\n",
       "      <th>_UserReputation</th>\n",
       "      <th>_UserUpVotes</th>\n",
       "      <th>_UserViews</th>\n",
       "      <th>_UserWebsiteUrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10397</td>\n",
       "      <td>329188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Report Generation in a Ruby Web Application I've been developing business apps, basically CRUD, in ASP.Net for years now, and am interested in learning another language and platform.\\nAfter a few trips to Borders and poking around a bit on the web, I have not found much dealing with generating r...</td>\n",
       "      <td></td>\n",
       "      <td>[ruby]</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mike Thomas</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4459</td>\n",
       "      <td>28036053</td>\n",
       "      <td>28036159.0</td>\n",
       "      <td>Objective-C/ARC ivar vs property I've been up and down the Google and the Stack and read many articles if not outright debates over ivars and properties.  But still, even after all this reading I remain confused.\\nI understand ivar's are private and properties are typically used to expose (well)...</td>\n",
       "      <td>@interface MyClass\\n@property(strong) NSMutableArray *myArray;\\n@end\\n\\n@interface MyClass\\n\\n-(instancetype)init {\\n\\n    if (self = [super init]) {\\n\\n        self.myArray = [NSMutableArray array];\\n\\n        // OR\\n\\n        // Will this NOT call the Setter?  Hence, leading\\n        // to pos...</td>\n",
       "      <td>[objective-c]</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>user1068477</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133304</td>\n",
       "      <td>2977779</td>\n",
       "      <td>2978212.0</td>\n",
       "      <td>add xml node to xml file with python I wonder if it is better add an element by opening file, search 'good place' and add string which contains xml code.\\nOr use some library... i have no idea. I know how can i get nodes and properties from xml through for example lxml but what's the simpliest a...</td>\n",
       "      <td></td>\n",
       "      <td>[python, xml]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>170961.0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>57071.0</td>\n",
       "      <td>170961.0</td>\n",
       "      <td>matiit</td>\n",
       "      <td>72.0</td>\n",
       "      <td>Cardiff, United Kingdom</td>\n",
       "      <td>None</td>\n",
       "      <td>5865.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106967</td>\n",
       "      <td>48491694</td>\n",
       "      <td>48491845.0</td>\n",
       "      <td>How get Environment Variables from lambda (nodejs aws-sdk) We can set up Environment Variables in aws-lambda for example via AWS SAM:\\n\\nHow can I get this variables from current lambda via Node JS AWS-SDK?\\n</td>\n",
       "      <td>Environment:\\n    Variables:\\n      TABLE_NAME: !Ref Table\\n</td>\n",
       "      <td>[amazon-web-services, aws-lambda, nodes]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6345354.0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>8458983.0</td>\n",
       "      <td>6345354.0</td>\n",
       "      <td>Max Vinogradov</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sumy, Sums'ka oblast, Ukraine</td>\n",
       "      <td>https://lh5.googleusercontent.com/-p49_IG9LR6c/AAAAAAAAAAI/AAAAAAAAABY/4XdJrDLMl10/photo.jpg?sz=128</td>\n",
       "      <td>150.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>https://www.linkedin.com/in/max-vinogradov/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94488</td>\n",
       "      <td>15663654</td>\n",
       "      <td>15664062.0</td>\n",
       "      <td>Get unique lines I'm creating graph in graphViz and I need every connection to be display only once, how to transform this input using linux commands?\\nINPUT\\n\\nDESIRED OUTPUT:\\n\\nso  equals to  and needs to be removed.\\nI tried  bot it didnt work with  delimiter and don't know how to check for ...</td>\n",
       "      <td>aa -- bb[label=xyz]\\nab -- bb[label=yzx]\\naa -- bb[label=zxy]\\nac -- ab[label=xyz]\\nbb -- aa[label=xzy]\\n\\naa -- bb[label=xyz]\\nab -- bb[label=yzx]\\nac -- ab[label=xyz]\\n\\naa -- bb\\nbb -- aa\\nsort -k1,2 -u -t[\\n[</td>\n",
       "      <td>[linux, bash, awk, unique, delimiter]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>619616.0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>307994.0</td>\n",
       "      <td>619616.0</td>\n",
       "      <td>Buksy</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Slovakia</td>\n",
       "      <td>None</td>\n",
       "      <td>5289.0</td>\n",
       "      <td>538.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>http://buksy.netkosice.sk/index.php</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         _PostId  _AcceptedAnswerId  \\\n",
       "10397     329188                NaN   \n",
       "4459    28036053         28036159.0   \n",
       "133304   2977779          2978212.0   \n",
       "106967  48491694         48491845.0   \n",
       "94488   15663654         15664062.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                              _Body  \\\n",
       "10397   Report Generation in a Ruby Web Application I've been developing business apps, basically CRUD, in ASP.Net for years now, and am interested in learning another language and platform.\\nAfter a few trips to Borders and poking around a bit on the web, I have not found much dealing with generating r...   \n",
       "4459    Objective-C/ARC ivar vs property I've been up and down the Google and the Stack and read many articles if not outright debates over ivars and properties.  But still, even after all this reading I remain confused.\\nI understand ivar's are private and properties are typically used to expose (well)...   \n",
       "133304  add xml node to xml file with python I wonder if it is better add an element by opening file, search 'good place' and add string which contains xml code.\\nOr use some library... i have no idea. I know how can i get nodes and properties from xml through for example lxml but what's the simpliest a...   \n",
       "106967                                                                                             How get Environment Variables from lambda (nodejs aws-sdk) We can set up Environment Variables in aws-lambda for example via AWS SAM:\\n\\nHow can I get this variables from current lambda via Node JS AWS-SDK?\\n   \n",
       "94488   Get unique lines I'm creating graph in graphViz and I need every connection to be display only once, how to transform this input using linux commands?\\nINPUT\\n\\nDESIRED OUTPUT:\\n\\nso  equals to  and needs to be removed.\\nI tried  bot it didnt work with  delimiter and don't know how to check for ...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                              _Code  \\\n",
       "10397                                                                                                                                                                                                                                                                                                                 \n",
       "4459    @interface MyClass\\n@property(strong) NSMutableArray *myArray;\\n@end\\n\\n@interface MyClass\\n\\n-(instancetype)init {\\n\\n    if (self = [super init]) {\\n\\n        self.myArray = [NSMutableArray array];\\n\\n        // OR\\n\\n        // Will this NOT call the Setter?  Hence, leading\\n        // to pos...   \n",
       "133304                                                                                                                                                                                                                                                                                                                \n",
       "106967                                                                                                                                                                                                                                                 Environment:\\n    Variables:\\n      TABLE_NAME: !Ref Table\\n   \n",
       "94488                                                                                          aa -- bb[label=xyz]\\nab -- bb[label=yzx]\\naa -- bb[label=zxy]\\nac -- ab[label=xyz]\\nbb -- aa[label=xzy]\\n\\naa -- bb[label=xyz]\\nab -- bb[label=yzx]\\nac -- ab[label=xyz]\\n\\naa -- bb\\nbb -- aa\\nsort -k1,2 -u -t[\\n[   \n",
       "\n",
       "                                           _Tags  _AnswerCount  _CommentCount  \\\n",
       "10397                                     [ruby]             5              0   \n",
       "4459                               [objective-c]             4              8   \n",
       "133304                             [python, xml]             2              0   \n",
       "106967  [amazon-web-services, aws-lambda, nodes]             1              0   \n",
       "94488      [linux, bash, awk, unique, delimiter]             3              0   \n",
       "\n",
       "        _FavoriteCount  _OwnerUserId _OwnerDisplayName  ...  _AccountId  \\\n",
       "10397              NaN           NaN       Mike Thomas  ...         NaN   \n",
       "4459               NaN           NaN       user1068477  ...         NaN   \n",
       "133304             NaN      170961.0              None  ...     57071.0   \n",
       "106967             1.0     6345354.0              None  ...   8458983.0   \n",
       "94488              NaN      619616.0              None  ...    307994.0   \n",
       "\n",
       "          _UserId _UserDisplayName  _UserDownVotes  \\\n",
       "10397         NaN             None             NaN   \n",
       "4459          NaN             None             NaN   \n",
       "133304   170961.0           matiit            72.0   \n",
       "106967  6345354.0   Max Vinogradov             0.0   \n",
       "94488    619616.0            Buksy             2.0   \n",
       "\n",
       "                        _UserLocation  \\\n",
       "10397                            None   \n",
       "4459                             None   \n",
       "133304        Cardiff, United Kingdom   \n",
       "106967  Sumy, Sums'ka oblast, Ukraine   \n",
       "94488                        Slovakia   \n",
       "\n",
       "                                                                                           _ProfileImageUrl  \\\n",
       "10397                                                                                                  None   \n",
       "4459                                                                                                   None   \n",
       "133304                                                                                                 None   \n",
       "106967  https://lh5.googleusercontent.com/-p49_IG9LR6c/AAAAAAAAAAI/AAAAAAAAABY/4XdJrDLMl10/photo.jpg?sz=128   \n",
       "94488                                                                                                  None   \n",
       "\n",
       "        _UserReputation _UserUpVotes _UserViews  \\\n",
       "10397               NaN          NaN        NaN   \n",
       "4459                NaN          NaN        NaN   \n",
       "133304           5865.0        243.0      407.0   \n",
       "106967            150.0         43.0       30.0   \n",
       "94488            5289.0        538.0      420.0   \n",
       "\n",
       "                                    _UserWebsiteUrl  \n",
       "10397                                          None  \n",
       "4459                                           None  \n",
       "133304                                               \n",
       "106967  https://www.linkedin.com/in/max-vinogradov/  \n",
       "94488           http://buksy.netkosice.sk/index.php  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 300)\n",
    "df.sample(50).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "# # Make each bin 100 count, since range is atm 500-2,000\n",
    "# df.groupby('_Tags').count()['_Body'].hist(bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the Data Initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 1000\n",
    "\n",
    "\n",
    "df['_Lower_Body'] = df['_Body'].apply(lambda x: x.lower())\n",
    "\n",
    "df_sample = df.sample(SAMPLE_SIZE, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n",
      "<class 'spacy.tokens.doc.Doc'> <class 'spacy.tokens.doc.Doc'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unsupported type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-319329f4354b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdoc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# df_sample['spacy'] = df['_Body'].apply(lambda x: nlp(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.similarity\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.vector_norm.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.ndarray.__add__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy.core._kernel.ufunc.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy.core._kernel._preprocess_args\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unsupported type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "spacy.prefer_gpu()\n",
    "\n",
    "# Download the spaCy english model\n",
    "spacy.cli.download('en_core_web_lg')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "doc1 = nlp(df['_Body'][0])\n",
    "doc2 = nlp(df['_Body'][1])\n",
    "\n",
    "print(type(doc1), type(doc2))\n",
    "doc1.similarity(doc2)\n",
    "\n",
    "# df_sample['spacy'] = df['_Body'].apply(lambda x: nlp(x))\n",
    "\n",
    "# ABSTAIN = -1\n",
    "\n",
    "# def keyword_lookup(x, keywords, label):\n",
    "    \n",
    "#     match = any(word in x.text for word in keywords)\n",
    "#     # print(keywords, match, label, x)\n",
    "#     if match:\n",
    "#         return label\n",
    "#     return ABSTAIN\n",
    "\n",
    "# keyword_lookup(doc, ['base64'], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data into Train/Test/Development Datasets\n",
    "\n",
    "We'll need to validate our labeling functions (LFs) in Snorkel, so we need train, test and __development__ datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test_dev, y_train, y_test_dev = train_test_split(\n",
    "    df_sample, \n",
    "    df_sample['_Index'], \n",
    "    test_size=0.3,\n",
    "    random_state=1337,\n",
    ")\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_test_dev,\n",
    "    y_test_dev,\n",
    "    test_size=0.66667,\n",
    "    random_state=1337,\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, X_dev.shape, y_train.shape, y_test.shape, y_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Function 1: Contains Tag\n",
    "\n",
    "The first labeling function we'll create is a keyword search. We'll look for whether the keyword is contained in the dataset. This would be helpful for a question about HTML with the tag `html` where `html` also appears in the body of the post.\n",
    "\n",
    "### Snorkel Proprocessors and LFs\n",
    "\n",
    "To do this we'll use a [`snorkel.preprocess.preprocessor`](https://snorkel.readthedocs.io/en/master/packages/_autosummary/preprocess/snorkel.preprocess.preprocessor.html#snorkel.preprocess.preprocessor) called [`snorkel.preprocess.nlp.SpacyPreprocessor`](https://snorkel.readthedocs.io/en/master/packages/_autosummary/preprocess/snorkel.preprocess.nlp.SpacyPreprocessor.html) to create [`spacy.Docs`](https://spacy.io/api/doc) from our string documents. This will give us the document in various forms: text, a list of [`spacy.Tokens`](https://spacy.io/api/token), and a [`spacy.Doc.vector`](https://spacy.io/api/doc#vector) representation. Additional features of `SnorkelPreprocessor`, `spacy.Doc` and `spacy.Token` make this incredibly useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging `SpacyPreprocessor`\n",
    "\n",
    "*If you have trouble instantiating `SpacyPreprocessor` because `en_core_web_sm` won't load, restart/run all the notebook and the problem should resolve itself.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "from snorkel.labeling import LabelingFunction\n",
    "\n",
    "ABSTAIN = -1\n",
    "\n",
    "\n",
    "spacy_processor = SpacyPreprocessor(\n",
    "    text_field='_Lower_Text',\n",
    "    doc_field='_Doc',\n",
    "    memoize=True,\n",
    "    gpu=True,\n",
    ")\n",
    "\n",
    "def keyword_lookup(x, keywords, label):\n",
    "    \n",
    "    match = any(word in x._Doc.text for word in keywords)\n",
    "    if match:\n",
    "        return label\n",
    "    return ABSTAIN\n",
    "\n",
    "def make_keyword_lf(keywords, label=ABSTAIN):\n",
    "    return LabelingFunction(\n",
    "        name=f\"keyword_{keywords}\",\n",
    "        f=keyword_lookup,\n",
    "        resources=dict(keywords=keywords, label=label),\n",
    "        pre=[spacy_processor],\n",
    "    )\n",
    "\n",
    "\n",
    "# For each keyword, split on hyphen and create an LF that detects if that tag is present in the data\n",
    "keyword_lfs = OrderedDict()\n",
    "for label_set, index in zip(df['_Tag'].unique(), df['_Index'].unique()):\n",
    "    for label in label_set.split('-'):\n",
    "        keyword_lfs[label] = make_keyword_lf(label, label=index)\n",
    "\n",
    "list(keyword_lfs.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply our LFs\n",
    "\n",
    "Let's try the first batch of tag keyword search LFs and see how they alone perform compared to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFAnalysis, PandasLFApplier\n",
    "\n",
    "X_train = X_train[['_Lower_Text']]\n",
    "X_dev   = X_dev[['_Lower_Text']]\n",
    "\n",
    "applier = PandasLFApplier(\n",
    "    lfs=keyword_lfs.values(),\n",
    ")\n",
    "\n",
    "L_train = applier.apply(df=X_train)\n",
    "L_dev   = applier.apply(df=X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary = LFAnalysis(L=L_dev, lfs=keyword_lfs.values()).lf_summary() # y_dev.as_matrix()\n",
    "# summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary[summary.index == 'keyword_base64']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply our LFs using PySpark\n",
    "\n",
    "For performance reasons, we want the parallel processing of PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from lib.utils import one_hot_encode\n",
    "\n",
    "PATHS = {\n",
    "    'bad_questions': {\n",
    "        'local': '../data/stackoverflow/Questions.Bad.{}.{}.parquet',\n",
    "        's3': 's3://stackoverflow-events/Questions.Bad.{}.{}.parquet',\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define a set of paths for each step for local and S3\n",
    "PATH_SET = 'local' # 's3'\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName('Deep Products - Create Weak Labels')\\\n",
    "    .config('spark.dynamicAllocation.enabled', True)\\\n",
    "    .config('spark.shuffle.service.enabled', True)\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "tag_limit, stratify_limit, bad_limit = 2000, 2000, 500\n",
    "\n",
    "bad_questions = spark.read.parquet(\n",
    "    PATHS['bad_questions'][PATH_SET].format(tag_limit, bad_limit)\n",
    ")\n",
    "\n",
    "# Redone from top for the moment\n",
    "from pyspark.sql import Row\n",
    "# from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "from snorkel.labeling import LabelingFunction\n",
    "from snorkel.labeling.apply.spark import SparkLFApplier\n",
    "from snorkel.labeling.lf.nlp_spark import SparkNLPLabelingFunction\n",
    "\n",
    "\n",
    "ABSTAIN = -1\n",
    "\n",
    "\n",
    "spacy_processor = SpacyPreprocessor(\n",
    "    text_field='_Body',\n",
    "    doc_field='_Doc',\n",
    "    memoize=True,\n",
    "    gpu=True,\n",
    ")\n",
    "\n",
    "def keyword_lookup(x, keywords, label):\n",
    "    \n",
    "    match = any(word in x._Doc.text for word in keywords)\n",
    "    \n",
    "    if match:\n",
    "        return label\n",
    "    return ABSTAIN\n",
    "\n",
    "\n",
    "# For each keyword, split on hyphen and create an LF that detects if that tag is present in the data\n",
    "keyword_lfs = OrderedDict()\n",
    "for label_set, index in zip(df['_Tag'].unique(), df['_Index'].unique()):\n",
    "    for label in label_set.split('-'):\n",
    "        keyword_lfs[label] = make_keyword_lf(label, label=index)\n",
    "\n",
    "lf_1 = list(keyword_lfs.items())\n",
    "\n",
    "LF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Vector Distances\n",
    "\n",
    "Now lets try using the [`vector`](https://spacy.io/api/doc#vector) feature of the [`spacy.Doc`](https://spacy.io/api/doc) which is returned in the `_Doc` field from the `SpacyPreprocessor` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
