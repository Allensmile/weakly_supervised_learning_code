{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Snorkel\n",
    "\n",
    "In this notebook we will use Snorkel to enrich our data such that tags with between 500-2,000 examples will be labeled using weak supervision to produce labels for enough examples to allow us to train an accurate full model that includes these new labels.\n",
    "\n",
    "More information about Snorkel can be found at [Snorkel.org](https://www.snorkel.org/) :) For a basic introduction to Snorkel, see the [Spam Tutorial](http://syndrome:8888/notebooks/snorkel-tutorials/spam/01_spam_tutorial.ipynb). For an introduction to Multi-Task Learning (MTL), see [Multi-Task Tutorial](http://syndrome:8888/notebooks/snorkel-tutorials/multitask/multitask_tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snorkel Introduction\n",
    "\n",
    "from collections import OrderedDict \n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import random\n",
    "import snorkel\n",
    "import tensorflow as tf\n",
    "\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_LIMIT = 2000\n",
    "BAD_LIMIT = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS = {\n",
    "    'final_tag_parquet': {\n",
    "        'local': '../data/stackoverflow/PerTag.Bad.{}.{}.parquet',\n",
    "        's3': 's3://stackoverflow-events/PerTag.Bad.{}.{}.parquet',\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define a set of paths for each step for local and S3\n",
    "PATH_SET = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Make sure we're running from the spam/ directory\n",
    "if os.path.basename(os.getcwd()) == \"snorkel-tutorials\":\n",
    "    os.chdir(\"spam\")\n",
    "\n",
    "# Turn off TensorFlow logging messages\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# For reproducibility\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"1337\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our Examples for Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = PATHS['final_tag_parquet'][PATH_SET].format(TAG_LIMIT, BAD_LIMIT)\n",
    "\n",
    "df = pd.read_parquet(\n",
    "    path, \n",
    "    columns=['_Body', '_Index', '_Tag'],\n",
    "    engine='pyarrow',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 300)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Make each bin 100 count, since range is atm 500-2,000\n",
    "df.groupby('_Tag').count()['_Body'].hist(bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the Data Initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 10000\n",
    "\n",
    "\n",
    "df['_Text'] = df['_Body'].apply(lambda x: ' '.join(x))\n",
    "df['_Lower_Text'] = df['_Text'].apply(lambda x: x.lower())\n",
    "\n",
    "# df['_Index_Plus'] = df['_Index'].apply(lambda x: x + 1)\n",
    "\n",
    "df_sample = df.sample(SAMPLE_SIZE, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data into Train/Test/Development Datasets\n",
    "\n",
    "We'll need to validate our labeling functions (LFs) in Snorkel, so we need train, test and __development__ datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test_dev, y_train, y_test_dev = train_test_split(\n",
    "    df_sample, \n",
    "    df_sample['_Index'], \n",
    "    test_size=0.3,\n",
    "    random_state=1337,\n",
    ")\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_test_dev,\n",
    "    y_test_dev,\n",
    "    test_size=0.66667,\n",
    "    random_state=1337,\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, X_dev.shape, y_train.shape, y_test.shape, y_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Function 1: Contains Tag\n",
    "\n",
    "The first labeling function we'll create is a keyword search. We'll look for whether the keyword is contained in the dataset. This would be helpful for a question about HTML with the tag `html` where `html` also appears in the body of the post.\n",
    "\n",
    "### Snorkel Proprocessors and LFs\n",
    "\n",
    "To do this we'll use a [Snorkel preprocessor](https://snorkel.readthedocs.io/en/master/packages/_autosummary/preprocess/snorkel.preprocess.preprocessor.html#snorkel.preprocess.preprocessor) that joins and lowercases the text before the LFs act on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the spaCy english model\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "from snorkel.labeling import LabelingFunction\n",
    "\n",
    "ABSTAIN = -1\n",
    "\n",
    "\n",
    "spacy_processor = SpacyPreprocessor(\n",
    "    text_field='_Lower_Text',\n",
    "    doc_field='_Doc',\n",
    "    memoize=True,\n",
    ")\n",
    "\n",
    "def keyword_lookup(x, keywords, label):\n",
    "    if any(word.lower() in x._Doc for word in keywords if len(word) > 2):\n",
    "        return label\n",
    "    return ABSTAIN\n",
    "\n",
    "def make_keyword_lf(keywords, label=ABSTAIN):\n",
    "    return LabelingFunction(\n",
    "        name=f\"keyword_{keywords}\",\n",
    "        f=keyword_lookup,\n",
    "        resources=dict(keywords=keywords, label=label),\n",
    "        pre=[spacy_processor]\n",
    "    )\n",
    "\n",
    "\n",
    "# For each keyword, split on hyphen and create an LF that detects if that tag is present in the data\n",
    "keyword_lfs = OrderedDict()\n",
    "for label_set, index in zip(df['_Tag'].unique(), df['_Index'].unique()):\n",
    "    for label in label_set.split('-'):\n",
    "        keyword_lfs[label] = make_keyword_lf(label, label=index)\n",
    "\n",
    "list(keyword_lfs.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply our LFs\n",
    "\n",
    "The"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFAnalysis, PandasLFApplier\n",
    "\n",
    "\n",
    "applier = PandasLFApplier(lfs=keyword_lfs.values())\n",
    "\n",
    "L_train = applier.apply(df=X_train)\n",
    "L_dev = applier.apply(df=X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = LFAnalysis(L=L_dev, lfs=keyword_lfs.values()).lf_summary(y_dev.as_matrix())\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.sort_values(by='Correct', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
